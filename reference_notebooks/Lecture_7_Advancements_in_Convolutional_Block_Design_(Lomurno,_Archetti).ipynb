{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyyx0cfzGF9"
      },
      "source": [
        "# **Artificial Neural Networks and Deep Learning**\n",
        "\n",
        "---\n",
        "\n",
        "## **Lecture 7: Advancements in Convolutional Block Design**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Ruszte0iwJ-i5VgTCApvJXz7yXWgnZzi\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdD_8Vyswkwf"
      },
      "source": [
        "## ‚öôÔ∏è Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_S1JfaW8bIN"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "!pip install torchview\n",
        "from torchview import draw_graph\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline\n",
        "\n",
        "# TensorBoard setup\n",
        "logs_dir = \"tensorboard_blocks\"\n",
        "!rm -rf {logs_dir}\n",
        "!mkdir -p {logs_dir}\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input and output dimensions\n",
        "input_shape = (3, 64, 64)\n",
        "output_shape = 10\n",
        "\n",
        "# Define the batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Initialize configuration for convolutional layers\n",
        "stack = 1\n",
        "filters = 32\n",
        "kernel_size = 3\n",
        "\n",
        "\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "print(f\"Output shape: {output_shape}\")\n",
        "print(f\"batch size: {BATCH_SIZE}\")\n",
        "print(f\"Stack: {stack}\")\n",
        "print(f\"Filters: {filters}\")\n",
        "print(f\"Kernel size: {kernel_size}\")"
      ],
      "metadata": {
        "id": "nLrhmuHmrxYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **First Convolutional Neural Network Block (AlexNet, 2012)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*bD_DMBtKwveuzIkQTwjKQQ.png\" width=\"800\"/>\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   First successful deep CNN for ImageNet\n",
        "*   Introduced ReLU to combat vanishing gradient\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Conv -> ReLU -> MaxPool sequence\n",
        "*   Multiple layers stacked sequentially\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Started the \"deep learning revolution\"\n",
        "*   Established basic CNN design patterns\n",
        "\n",
        "**üìú Paper:** [\"ImageNet Classification with Deep Convolutional Neural Networks\", Krizhevsky et al.](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "u2arNe9atvaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicCNNBlock(nn.Module):\n",
        "    \"\"\"Basic CNN block with Conv -> ReLU -> MaxPool pattern (AlexNet style).\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size=3, padding='same',\n",
        "                 downsample=True, stack=2):\n",
        "        super().__init__()\n",
        "\n",
        "        layers_list = []\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for i in range(stack):\n",
        "            layers_list.append(\n",
        "                nn.Conv2d(current_channels, filters, kernel_size, padding=padding)\n",
        "            )\n",
        "            layers_list.append(nn.ReLU())\n",
        "            current_channels = filters\n",
        "\n",
        "        if downsample:\n",
        "            layers_list.append(nn.MaxPool2d(2))\n",
        "\n",
        "        self.block = nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class BasicCNNModel(nn.Module):\n",
        "    \"\"\"Complete model using BasicCNNBlock.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = BasicCNNBlock(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            downsample=True,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        # Calculate flattened size\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            dummy_out = self.block0(dummy)\n",
        "            flatten_size = dummy_out.view(1, -1).shape[1]\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(flatten_size, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "aSb4owGstTcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the Basic CNN model\n",
        "basic_cnn = BasicCNNModel(input_shape, output_shape, filters, kernel_size, stack).to(device)\n",
        "summary(basic_cnn, input_size=input_shape)\n",
        "model_graph = draw_graph(basic_cnn, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "CXrRjILF-1-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Global Average Pooling (NiN, 2013)**\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/363231491/figure/fig5/AS:11431281179419529@1691187457237/Illustration-of-global-average-pooling-GAP.png\" width=\"800\"/>\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Replaced Flatten and Dense layers\n",
        "*   Enforced correspondence between feature maps and categories\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Global spatial average of each feature map\n",
        "*   Direct feature-to-category mapping\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Dramatic parameter reduction\n",
        "*   Better generalization with fewer parameters\n",
        "\n",
        "**üìú Paper:** [\"Network In Network\", Lin et al.](https://arxiv.org/pdf/1312.4400)\n",
        "\n"
      ],
      "metadata": {
        "id": "RkqPKmkQra2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAPModel(nn.Module):\n",
        "    \"\"\"Model using Global Average Pooling instead of Flatten + Dense.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = BasicCNNBlock(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            downsample=True,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "cx3Mww4hqtd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the GAP model\n",
        "gap_model = GAPModel(input_shape, output_shape, filters, kernel_size, stack).to(device)\n",
        "summary(gap_model, input_size=input_shape)\n",
        "model_graph = draw_graph(gap_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "qsRkQodF_K27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Inception Block (GoogLeNet, 2014)**\n",
        "\n",
        "<img src=\"https://ar5iv.labs.arxiv.org/html/1707.07128/assets/googlenetInception.png\" width=\"800\"/>\n",
        "\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Multi-scale feature processing\n",
        "*   Winner of ILSVRC 2014\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Parallel paths with different kernels\n",
        "*   1x1 bottleneck for efficiency\n",
        "*   Feature concatenation\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Established multi-path processing\n",
        "*   Introduced 1x1 bottleneck concept\n",
        "\n",
        "**üìú Paper:** [\"Going deeper with convolutions\", Szegedy et al.](https://arxiv.org/pdf/1409.4842)"
      ],
      "metadata": {
        "id": "_Cl1WzXxuOLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    \"\"\"Original Inception block (2014) with parallel convolution paths.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, downsample=True, stack=2):\n",
        "        super().__init__()\n",
        "        self.stack = stack\n",
        "        self.downsample = downsample\n",
        "\n",
        "        # Build stacked inception modules\n",
        "        self.inception_modules = nn.ModuleList()\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for s in range(stack):\n",
        "            module = nn.ModuleDict({\n",
        "                # 1x1 path\n",
        "                'conv1': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 4, 1, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # 3x3 path with reduction\n",
        "                'conv3_reduce': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 8, 1, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                'conv3': nn.Sequential(\n",
        "                    nn.Conv2d(filters // 8, filters // 4, 3, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # 5x5 path with reduction\n",
        "                'conv5_reduce': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 12, 1, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                'conv5': nn.Sequential(\n",
        "                    nn.Conv2d(filters // 12, filters // 4, 5, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # Pool path\n",
        "                'pool': nn.MaxPool2d(3, stride=1, padding=1),\n",
        "                'pool_proj': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 4, 1, padding='same'),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            })\n",
        "            self.inception_modules.append(module)\n",
        "            current_channels = filters  # After concatenation\n",
        "\n",
        "        if downsample:\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.inception_modules:\n",
        "            conv1 = module['conv1'](x)\n",
        "            conv3 = module['conv3'](module['conv3_reduce'](x))\n",
        "            conv5 = module['conv5'](module['conv5_reduce'](x))\n",
        "            pool_proj = module['pool_proj'](module['pool'](x))\n",
        "            x = torch.cat([conv1, conv3, conv5, pool_proj], dim=1)\n",
        "\n",
        "        if self.downsample:\n",
        "            x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionModel(nn.Module):\n",
        "    \"\"\"Complete model using InceptionBlock.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = InceptionBlock(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            downsample=True,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "hafbI5_Rqta_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the Inception model\n",
        "inception_model = InceptionModel(input_shape, output_shape, filters, stack).to(device)\n",
        "summary(inception_model, input_size=input_shape)\n",
        "model_graph = draw_graph(inception_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "Kpq352hp_nN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Batch Normalization (Inception Block with BN, 2015)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:898/0*pSSzicm1IH4hXOHc.png\" width=\"800\"/>\n",
        "\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Normalized activations in each layer\n",
        "*   Reduced internal covariate shift\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Normalize: $\\hat{x} = \\frac{x-\\mu_B}{\\sqrt{\\sigma^2_B+\\epsilon}}$\n",
        "*   Scale and shift: $y = \\gamma\\hat{x} + \\beta$\n",
        "*   Placed before activation\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Enabled much faster training\n",
        "*   Reduced sensitivity to initialization\n",
        "*   Became standard in modern networks\n",
        "\n",
        "**üìú Paper:** [\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\", Ioffe and Szegedy](https://arxiv.org/pdf/1502.03167)"
      ],
      "metadata": {
        "id": "IpTP8L0z2O9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlockBN(nn.Module):\n",
        "    \"\"\"Inception block with Batch Normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, downsample=True, stack=2):\n",
        "        super().__init__()\n",
        "        self.stack = stack\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.inception_modules = nn.ModuleList()\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for s in range(stack):\n",
        "            module = nn.ModuleDict({\n",
        "                # 1x1 path with BN\n",
        "                'conv1': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 4, 1, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 4),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # 3x3 path with reduction and BN\n",
        "                'conv3_reduce': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 8, 1, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 8),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                'conv3': nn.Sequential(\n",
        "                    nn.Conv2d(filters // 8, filters // 4, 3, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 4),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # 5x5 path with reduction and BN\n",
        "                'conv5_reduce': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 12, 1, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 12),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                'conv5': nn.Sequential(\n",
        "                    nn.Conv2d(filters // 12, filters // 4, 5, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 4),\n",
        "                    nn.ReLU()\n",
        "                ),\n",
        "                # Pool path with BN\n",
        "                'pool': nn.MaxPool2d(3, stride=1, padding=1),\n",
        "                'pool_proj': nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters // 4, 1, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters // 4),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            })\n",
        "            self.inception_modules.append(module)\n",
        "            current_channels = filters\n",
        "\n",
        "        if downsample:\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.inception_modules:\n",
        "            conv1 = module['conv1'](x)\n",
        "            conv3 = module['conv3'](module['conv3_reduce'](x))\n",
        "            conv5 = module['conv5'](module['conv5_reduce'](x))\n",
        "            pool_proj = module['pool_proj'](module['pool'](x))\n",
        "            x = torch.cat([conv1, conv3, conv5, pool_proj], dim=1)\n",
        "\n",
        "        if self.downsample:\n",
        "            x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InceptionBNModel(nn.Module):\n",
        "    \"\"\"Complete model using InceptionBlockBN.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = InceptionBlockBN(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            downsample=True,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "H1lCnHdi2PG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the Inception with BN model\n",
        "inception_bn_model = InceptionBNModel(input_shape, output_shape, filters, stack).to(device)\n",
        "summary(inception_bn_model, input_size=input_shape)\n",
        "model_graph = draw_graph(inception_bn_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "L0sKNVCi_5vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Residual Block (ResNet, 2015)**\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png\" width=\"800\"/>\n",
        "\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Enabled 1000+ layer networks\n",
        "*   Winner of ILSVRC 2015\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Skip connection: F(x) + x\n",
        "*   Two conv layers with BN and ReLU\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Solved deep network degradation\n",
        "*   Revolutionized network design\n",
        "\n",
        "**üìú Paper:** [\"Deep Residual Learning for Image Recognition\", He et al.](https://arxiv.org/pdf/1512.03385)"
      ],
      "metadata": {
        "id": "P1bSqyAL0RzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with skip connections.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size=3, downsample=True, stack=2):\n",
        "        super().__init__()\n",
        "        self.stack = stack\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.residual_units = nn.ModuleList()\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for s in range(stack):\n",
        "            unit = nn.ModuleDict({\n",
        "                'conv1': nn.Conv2d(current_channels, filters, kernel_size, padding='same', bias=False),\n",
        "                'bn1': nn.BatchNorm2d(filters),\n",
        "                'conv2': nn.Conv2d(filters, filters, kernel_size, padding='same', bias=False),\n",
        "                'bn2': nn.BatchNorm2d(filters),\n",
        "            })\n",
        "\n",
        "            # Projection for skip connection if dimensions don't match\n",
        "            if current_channels != filters:\n",
        "                unit['proj'] = nn.Sequential(\n",
        "                    nn.Conv2d(current_channels, filters, 1, padding='same', bias=False),\n",
        "                    nn.BatchNorm2d(filters)\n",
        "                )\n",
        "\n",
        "            self.residual_units.append(unit)\n",
        "            current_channels = filters\n",
        "\n",
        "        if downsample:\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for unit in self.residual_units:\n",
        "            skip = x\n",
        "\n",
        "            # Main path\n",
        "            x = unit['conv1'](x)\n",
        "            x = unit['bn1'](x)\n",
        "            x = F.relu(x)\n",
        "\n",
        "            x = unit['conv2'](x)\n",
        "            x = unit['bn2'](x)\n",
        "\n",
        "            # Adjust skip connection if needed\n",
        "            if 'proj' in unit:\n",
        "                skip = unit['proj'](skip)\n",
        "\n",
        "            # Add skip connection and apply activation\n",
        "            x = F.relu(x + skip)\n",
        "\n",
        "        if self.downsample:\n",
        "            x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNetModel(nn.Module):\n",
        "    \"\"\"Complete model using ResidualBlock.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv0 = nn.Conv2d(input_shape[0], filters, kernel_size, padding='same', bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(filters)\n",
        "\n",
        "        # Residual block\n",
        "        self.block0 = ResidualBlock(\n",
        "            in_channels=filters,\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            downsample=False,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "-a4znc8WqtYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the ResNet model\n",
        "resnet_model = ResNetModel(input_shape, output_shape, filters, kernel_size, stack).to(device)\n",
        "summary(resnet_model, input_size=input_shape)\n",
        "model_graph = draw_graph(resnet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "R51CF_fYAM8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Squeeze-and-Excitation Block (SENet, 2017)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*QK1TVTasgdRYpVC31CuPyA.png\" width=\"800\"/>\n",
        "\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Channel \"attention\" mechanism\n",
        "*   Winner of ILSVRC 2017\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   Squeeze: global pooling\n",
        "*   Excitation: channel recalibration\n",
        "*   Feature rescaling\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Introduced \"attention\" in CNNs\n",
        "*   Minimal overhead, significant gain\n",
        "\n",
        "**üìú Paper:** [\"Squeeze-and-Excitation Networks\", Hu et al.](https://arxiv.org/pdf/1709.01507)"
      ],
      "metadata": {
        "id": "IfLhPch5-LPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block.\"\"\"\n",
        "\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "\n",
        "        # Squeeze\n",
        "        se = self.squeeze(x).view(b, c)\n",
        "\n",
        "        # Excitation\n",
        "        se = self.excitation(se).view(b, c, 1, 1)\n",
        "\n",
        "        # Scale\n",
        "        return x * se\n",
        "\n",
        "\n",
        "class SENetBlock(nn.Module):\n",
        "    \"\"\"Convolutional block with Squeeze-and-Excitation.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size=3, downsample=True, stack=2):\n",
        "        super().__init__()\n",
        "        self.stack = stack\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for s in range(stack):\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv2d(current_channels, filters, kernel_size, padding='same', bias=False),\n",
        "                nn.BatchNorm2d(filters),\n",
        "                nn.ReLU(),\n",
        "                SEBlock(filters)\n",
        "            )\n",
        "            self.conv_layers.append(layer)\n",
        "            current_channels = filters\n",
        "\n",
        "        if downsample:\n",
        "            self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        if self.downsample:\n",
        "            x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SENetModel(nn.Module):\n",
        "    \"\"\"Complete model using SENetBlock.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = SENetBlock(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            downsample=False,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "uproKfNFqtVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the SENet model\n",
        "senet_model = SENetModel(input_shape, output_shape, filters, kernel_size, stack).to(device)\n",
        "summary(senet_model, input_size=input_shape)\n",
        "model_graph = draw_graph(senet_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "_U9hKkOEAYVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Inverted Residual Bottleneck with SE (MobileNetV3, 2019)**\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/378806327/figure/fig5/AS:11431281232045939@1711587010842/MobileNetV3-network-structure.jpg\" width=\"600\"/>\n",
        "\n",
        "\n",
        "---\n",
        "**Key Features and Achievements**\n",
        "\n",
        "\n",
        "*   Pioneered platform-aware Neural Architecture Search (NAS)\n",
        "*   Introduced hardware-aware network design\n",
        "*   Combined manual design with automated search\n",
        "*   Optimized for mobile inference latency\n",
        "\n",
        "**Key building block:**\n",
        "\n",
        "*   \"Enhanced\" Inverted Residual Block (Expansion ratio tuned per block, SE module redesigned for efficiency, Hard-Swish activation function(h-swish))\n",
        "*   Efficient last stage design (Reduced channels in first layer, moved SE to cheaper layers, platform-aware operator selection)\n",
        "\n",
        "**Impact:**\n",
        "\n",
        "*   Set new SOTA for mobile networks\n",
        "*   Demonstrated successful NAS and human design fusion\n",
        "*   Showed importance of hardware-aware architecture design\n",
        "*   Influenced automated architecture search methods\n",
        "\n",
        "**üìú Paper:** [\"Searching for MobileNetV3\", Howard et al.](https://arxiv.org/pdf/1905.02244)\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1EcBp60nEorTDLROT_1L4a4Mvw9kLdKqd\" width=\"300\"/>"
      ],
      "metadata": {
        "id": "xdKdd0GH__Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HardSwish(nn.Module):\n",
        "    \"\"\"Hard-Swish activation function.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * F.relu6(x + 3.0) / 6.0\n",
        "\n",
        "\n",
        "class HardSigmoid(nn.Module):\n",
        "    \"\"\"Hard-Sigmoid activation function.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu6(x + 3.0) / 6.0\n",
        "\n",
        "\n",
        "class InvertedResidualSE(nn.Module):\n",
        "    \"\"\"Single MobileNetV3 Inverted Residual Block with SE.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 expansion_factor=6, use_hard_swish=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_residual = (stride == 1 and in_channels == out_channels)\n",
        "        expanded_channels = in_channels * expansion_factor\n",
        "\n",
        "        activation = HardSwish() if use_hard_swish else nn.ReLU()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Expansion phase (only if expansion_factor > 1)\n",
        "        if expansion_factor != 1:\n",
        "            layers.extend([\n",
        "                nn.Conv2d(in_channels, expanded_channels, 1, bias=False),\n",
        "                nn.BatchNorm2d(expanded_channels),\n",
        "                activation\n",
        "            ])\n",
        "\n",
        "        # Depthwise convolution\n",
        "        layers.extend([\n",
        "            nn.Conv2d(expanded_channels, expanded_channels, kernel_size,\n",
        "                     stride=stride, padding=kernel_size//2, groups=expanded_channels, bias=False),\n",
        "            nn.BatchNorm2d(expanded_channels),\n",
        "            activation\n",
        "        ])\n",
        "\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        se_channels = max(1, expanded_channels // 4)\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(expanded_channels, se_channels, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(se_channels, expanded_channels, 1),\n",
        "            HardSigmoid()\n",
        "        )\n",
        "\n",
        "        # Projection phase\n",
        "        self.project = nn.Sequential(\n",
        "            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = x * self.se(x)\n",
        "        x = self.project(x)\n",
        "\n",
        "        if self.use_residual:\n",
        "            x = x + residual\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileNetV3Block(nn.Module):\n",
        "    \"\"\"MobileNetV3 block with multiple Inverted Residual units.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, kernel_size=3, downsample=True,\n",
        "                 stack=1, use_hard_swish=True):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = []\n",
        "        current_channels = in_channels\n",
        "\n",
        "        for s in range(stack):\n",
        "            # Determine expansion factor\n",
        "            expansion_factor = 1 if current_channels == filters else 6\n",
        "            stride = 2 if (downsample and s == 0) else 1\n",
        "\n",
        "            blocks.append(InvertedResidualSE(\n",
        "                in_channels=current_channels,\n",
        "                out_channels=filters,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                expansion_factor=expansion_factor,\n",
        "                use_hard_swish=use_hard_swish\n",
        "            ))\n",
        "\n",
        "            current_channels = filters\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "\n",
        "class MobileNetV3Model(nn.Module):\n",
        "    \"\"\"Complete model using MobileNetV3Block.\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_shape, filters=32, kernel_size=3, stack=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = MobileNetV3Block(\n",
        "            in_channels=input_shape[0],\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            downsample=False,\n",
        "            stack=stack\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(filters, output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "mGI4vdIrqtSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the MobileNetV3 model\n",
        "mobilenetv3_model = MobileNetV3Model(input_shape, output_shape, filters, kernel_size, stack).to(device)\n",
        "summary(mobilenetv3_model, input_size=input_shape)\n",
        "model_graph = draw_graph(mobilenetv3_model, input_size=(BATCH_SIZE,)+input_shape, expand_nested=True, depth=5)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "K8CwErd1Ak1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4tWF6oUQFqH"
      },
      "source": [
        "#  \n",
        "<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n",
        "\n",
        "##### Connect with us:\n",
        "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n",
        "- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n",
        "\n",
        "##### Contributors:\n",
        "- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n",
        "- **Alberto Archetti**: alberto.archetti@polimi.it\n",
        "- **Roberto Basla**: roberto.basla@polimi.it\n",
        "- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n",
        "\n",
        "```\n",
        "   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}