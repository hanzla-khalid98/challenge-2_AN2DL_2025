{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831248ae",
   "metadata": {},
   "source": [
    "## Config & Paths - Colab Pro Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION & PATHS =====\n",
    "# Set base path for Colab vs local\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/KaggleChallenge2'\n",
    "else:\n",
    "    BASE_PATH = os.path.dirname(os.path.abspath('.'))\n",
    "\n",
    "# Verify base path exists\n",
    "if IN_COLAB and not os.path.exists(BASE_PATH):\n",
    "    print(f'âŒ BASE_PATH does not exist: {BASE_PATH}')\n",
    "    print(f'\\nAvailable folders in /content/drive/MyDrive/:')\n",
    "    import subprocess\n",
    "    result = subprocess.run(['ls', '/content/drive/MyDrive/'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(f'âœ“ BASE_PATH verified: {BASE_PATH}')\n",
    "\n",
    "DATASET_DIR = os.path.join(BASE_PATH, 'dataset')\n",
    "TRAIN_DATA_PATH = os.path.join(DATASET_DIR, 'train_data')\n",
    "TEST_DATA_PATH = os.path.join(DATASET_DIR, 'test_data')\n",
    "LABELS_FILE = os.path.join(DATASET_DIR, 'train_labels.csv')\n",
    "\n",
    "# Verify dataset paths\n",
    "print(f'\\nDataset paths:')\n",
    "print(f'  Dataset dir: {DATASET_DIR} (exists: {os.path.exists(DATASET_DIR)})')\n",
    "print(f'  Train data: {TRAIN_DATA_PATH} (exists: {os.path.exists(TRAIN_DATA_PATH)})')\n",
    "print(f'  Test data: {TEST_DATA_PATH} (exists: {os.path.exists(TEST_DATA_PATH)})')\n",
    "print(f'  Labels file: {LABELS_FILE} (exists: {os.path.exists(LABELS_FILE)})')\n",
    "\n",
    "# Cache directory\n",
    "if IN_COLAB:\n",
    "    CACHE_DIR = os.path.join(BASE_PATH, 'cache_convnext_base')\n",
    "else:\n",
    "    CACHE_DIR = 'data_cache_base'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "X_TRAIN_CACHE = os.path.join(CACHE_DIR, 'X_train_384.npy')\n",
    "Y_TRAIN_CACHE = os.path.join(CACHE_DIR, 'y_train.npy')\n",
    "X_TEST_CACHE = os.path.join(CACHE_DIR, 'X_test_384.npy')\n",
    "TEST_FILES_CACHE = os.path.join(CACHE_DIR, 'test_files.pkl')\n",
    "\n",
    "# Models directory\n",
    "MODELS_DIR = os.path.join(BASE_PATH, 'models_convnext_base')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Submissions directory\n",
    "SUBMISSION_DIR = os.path.join(BASE_PATH, 'submissions')\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "\n",
    "# ===== TRAINING CONFIGURATION - COLAB PRO (NO CONSTRAINTS) =====\n",
    "TARGET_SIZE = (384, 384)  # Higher resolution for better features\n",
    "BATCH_SIZE = 64  # Large batch for stable gradients\n",
    "EPOCHS = 150  # Longer training with more data\n",
    "KFOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print('\\nâœ“ Configuration loaded')\n",
    "print(f'  Running in: {\"Colab Pro\" if IN_COLAB else \"Local\"}')\n",
    "print(f'  Cache dir: {CACHE_DIR}')\n",
    "print(f'  Models dir: {MODELS_DIR}')\n",
    "print(f'  Target size: {TARGET_SIZE}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Epochs: {EPOCHS}')\n",
    "print(f'  K-Folds: {KFOLDS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32320a2",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image loading utility\n",
    "def load_image(path):\n",
    "    \"\"\"Load image, resize, and normalize to [0, 1].\"\"\"\n",
    "    img = Image.open(path).convert('RGB').resize(TARGET_SIZE)\n",
    "    return np.asarray(img, dtype=np.float32) / 255.0\n",
    "\n",
    "\n",
    "# Focal Loss with class-balanced weighting\n",
    "class FocalLossCB(keras.losses.Loss):\n",
    "    \"\"\"Focal loss with per-class alpha weighting.\"\"\"\n",
    "    def __init__(self, alpha_vec, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha_vec = tf.constant(alpha_vec, dtype=tf.float32)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        pt = tf.reduce_sum(y_true * y_pred, axis=1)\n",
    "        ce = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1)\n",
    "        alpha = tf.reduce_sum(y_true * self.alpha_vec, axis=1)\n",
    "        focal = alpha * tf.pow(1.0 - pt, self.gamma) * ce\n",
    "        return tf.reduce_mean(focal)\n",
    "\n",
    "\n",
    "# MixUp augmentation for tf.data\n",
    "def make_mixup_dataset(X, y, batch_size, num_classes, alpha=0.4, shuffle=True):\n",
    "    \"\"\"Create dataset with MixUp augmentation. y should be integer class labels.\"\"\"\n",
    "    y_onehot = keras.utils.to_categorical(y, num_classes=num_classes).astype(np.float32)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y_onehot))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(X), reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    def _mix(batch_x, batch_y):\n",
    "        beta = tf.random.uniform((), 0, 1)\n",
    "        lam = tf.cast(tf.maximum(beta, 1.0 - beta), tf.float32)\n",
    "        idx = tf.random.shuffle(tf.range(tf.shape(batch_x)[0]))\n",
    "        x2 = tf.gather(batch_x, idx)\n",
    "        y2 = tf.gather(batch_y, idx)\n",
    "        mixed_x = lam * batch_x + (1.0 - lam) * x2\n",
    "        mixed_y = lam * batch_y + (1.0 - lam) * y2\n",
    "        return mixed_x, mixed_y\n",
    "    \n",
    "    return ds.map(_mix, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print('âœ“ Utility functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9c553",
   "metadata": {},
   "source": [
    "## Load & Cache Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a00fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and fit encoder\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_df['sample_index'] = labels_df['sample_index'].astype(str)\n",
    "\n",
    "label_encoder = LabelEncoder().fit(labels_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f'âœ“ num_classes: {num_classes}')\n",
    "print(f'  Classes: {label_encoder.classes_}')\n",
    "\n",
    "# Build label mapping\n",
    "def normalize_filename(s):\n",
    "    s = str(s)\n",
    "    if not s.startswith('img_'):\n",
    "        s = 'img_' + s\n",
    "    if not (s.endswith('.png') or s.endswith('.jpg') or s.endswith('.jpeg')):\n",
    "        s = s + '.png'\n",
    "    return s\n",
    "\n",
    "labels_df['key'] = labels_df['sample_index'].apply(normalize_filename)\n",
    "labels_map = dict(zip(labels_df['key'], labels_df['label']))\n",
    "\n",
    "print(f'âœ“ Label encoder fitted with {len(labels_map)} labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1703ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or cache training data\n",
    "reload_from_disk = False\n",
    "if os.path.exists(X_TRAIN_CACHE) and os.path.exists(Y_TRAIN_CACHE):\n",
    "    X_train = np.load(X_TRAIN_CACHE)\n",
    "    y_train = np.load(Y_TRAIN_CACHE)\n",
    "    if X_train.shape[1:3] != TARGET_SIZE:\n",
    "        print(f'Cached X_train shape {X_train.shape[1:3]} != TARGET_SIZE {TARGET_SIZE}; reloading...')\n",
    "        reload_from_disk = True\n",
    "        try:\n",
    "            os.remove(X_TRAIN_CACHE)\n",
    "            os.remove(Y_TRAIN_CACHE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(f'âœ“ Loaded from cache: X_train={X_train.shape}, y_train={y_train.shape}')\n",
    "\n",
    "if not os.path.exists(X_TRAIN_CACHE) or reload_from_disk:\n",
    "    print('Loading X_train and y_train from disk...')\n",
    "    train_files = sorted([f for f in os.listdir(TRAIN_DATA_PATH) if f.startswith('img_')])\n",
    "    train_files = [f for f in train_files if f in labels_map]\n",
    "    print(f'  Total train files: {len(train_files)}')\n",
    "\n",
    "    y_train = label_encoder.transform([labels_map[f] for f in train_files])\n",
    "    X_train = np.zeros((len(train_files), *TARGET_SIZE, 3), dtype=np.float32)\n",
    "    for i, fname in enumerate(tqdm(train_files, desc='Loading train images')):\n",
    "        X_train[i] = load_image(os.path.join(TRAIN_DATA_PATH, fname))\n",
    "\n",
    "    np.save(X_TRAIN_CACHE, X_train)\n",
    "    np.save(Y_TRAIN_CACHE, y_train)\n",
    "    print(f'âœ“ Cached to {CACHE_DIR}')\n",
    "\n",
    "print(f'âœ“ Train set: X={X_train.shape}, y={y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or cache test data\n",
    "reload_from_disk = False\n",
    "if os.path.exists(X_TEST_CACHE) and os.path.exists(TEST_FILES_CACHE):\n",
    "    X_test = np.load(X_TEST_CACHE)\n",
    "    with open(TEST_FILES_CACHE, 'rb') as f:\n",
    "        test_files = pickle.load(f)\n",
    "    if X_test.shape[1:3] != TARGET_SIZE:\n",
    "        print(f'Cached X_test shape {X_test.shape[1:3]} != TARGET_SIZE {TARGET_SIZE}; reloading...')\n",
    "        reload_from_disk = True\n",
    "        try:\n",
    "            os.remove(X_TEST_CACHE)\n",
    "            os.remove(TEST_FILES_CACHE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        print(f'âœ“ Loaded from cache: X_test={X_test.shape}, test_files={len(test_files)}')\n",
    "\n",
    "if not os.path.exists(X_TEST_CACHE) or reload_from_disk:\n",
    "    print('Loading X_test and test_files from disk...')\n",
    "    test_files = sorted([f for f in os.listdir(TEST_DATA_PATH) if f.startswith('img_')])\n",
    "    print(f'  Total test files: {len(test_files)}')\n",
    "\n",
    "    X_test = np.zeros((len(test_files), *TARGET_SIZE, 3), dtype=np.float32)\n",
    "    for idx, filename in enumerate(tqdm(test_files, desc='Loading test images')):\n",
    "        path = os.path.join(TEST_DATA_PATH, filename)\n",
    "        try:\n",
    "            X_test[idx] = load_image(path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to load {path}: {e}')\n",
    "\n",
    "    np.save(X_TEST_CACHE, X_test)\n",
    "    with open(TEST_FILES_CACHE, 'wb') as f:\n",
    "        pickle.dump(test_files, f)\n",
    "    print(f'âœ“ Cached to {CACHE_DIR}')\n",
    "\n",
    "print(f'âœ“ Test set: X={X_test.shape}, files={len(test_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1630342b",
   "metadata": {},
   "source": [
    "## Model Architecture - ConvNeXt-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_convnext_base_model(num_classes):\n",
    "    \"\"\"\n",
    "    ConvNeXt-Base with aggressive fine-tuning for maximum accuracy.\n",
    "    - Unfreezes top 80% of layers (more capacity than Tiny)\n",
    "    - Deep classification head (1024â†’512â†’256)\n",
    "    - Strong augmentation during training only\n",
    "    \"\"\"\n",
    "    # Load ConvNeXt-Base base\n",
    "    base = keras.applications.ConvNeXtBase(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(*TARGET_SIZE, 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Unfreeze top 80% of layers for aggressive fine-tuning\n",
    "    total_layers = len(base.layers)\n",
    "    freeze_until = int(total_layers * 0.20)\n",
    "    \n",
    "    base.trainable = True\n",
    "    for i, layer in enumerate(base.layers):\n",
    "        layer.trainable = (i >= freeze_until)\n",
    "    \n",
    "    trainable_count = sum([1 for layer in base.layers if layer.trainable])\n",
    "    print(f'ConvNeXt-Base: {total_layers} layers, {trainable_count} trainable ({trainable_count/total_layers*100:.1f}%)')\n",
    "    \n",
    "    # Model with strong augmentation (training only)\n",
    "    inputs = keras.Input(shape=(*TARGET_SIZE, 3))\n",
    "    \n",
    "    # Aggressive augmentation for training\n",
    "    x = layers.RandomFlip('horizontal_and_vertical')(inputs, training=True)\n",
    "    x = layers.RandomRotation(0.25)(x, training=True)  # Â±25% rotation\n",
    "    x = layers.RandomZoom(0.25)(x, training=True)  # Â±25% zoom\n",
    "    x = layers.RandomTranslation(0.2, 0.2)(x, training=True)  # Â±20% translation\n",
    "    x = layers.RandomContrast(0.3)(x, training=True)  # Contrast jitter\n",
    "    \n",
    "    # ConvNeXt preprocessing\n",
    "    x = keras.applications.convnext.preprocess_input(x)\n",
    "    \n",
    "    # Backbone\n",
    "    x = base(x, training=True)\n",
    "    \n",
    "    # Deep classification head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(1024, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs, name='ConvNeXt_Base_FT')\n",
    "\n",
    "\n",
    "# Build and test model\n",
    "model_test = build_convnext_base_model(num_classes)\n",
    "print(f'âœ“ ConvNeXt-Base model built')\n",
    "print(f'  Total params: {model_test.count_params():,}')\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model_test.trainable_weights])\n",
    "print(f'  Trainable params: {trainable_params:,} ({trainable_params/model_test.count_params()*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b312cb",
   "metadata": {},
   "source": [
    "## K-Fold Training - ConvNeXt-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6feacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=KFOLDS, shuffle=True, random_state=SEED)\n",
    "weight_paths = []\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'Starting {KFOLDS}-fold cross-validation with ConvNeXt-Base')\n",
    "print('=' * 60)\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    print(f'\\nðŸ“Š FOLD {fold_num}/{KFOLDS}')\n",
    "    print('-' * 40)\n",
    "    \n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    print(f'Train samples: {len(X_tr)}, Val samples: {len(X_val)}')\n",
    "    \n",
    "    # Class-balanced focal loss weights (effective number)\n",
    "    class_counts = np.bincount(y_tr, minlength=num_classes)\n",
    "    print(f'Class distribution: {class_counts}')\n",
    "    \n",
    "    beta = 0.9999\n",
    "    effective_num = 1.0 - np.power(beta, class_counts)\n",
    "    weights_cb = (1.0 - beta) / (effective_num + 1e-7)\n",
    "    weights_cb = weights_cb / weights_cb.sum() * num_classes\n",
    "    print(f'Class weights: {weights_cb.round(3)}')\n",
    "    \n",
    "    # MixUp training dataset\n",
    "    train_ds = make_mixup_dataset(X_tr, y_tr, batch_size=BATCH_SIZE, num_classes=num_classes, alpha=0.5, shuffle=True)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, keras.utils.to_categorical(y_val, num_classes)))\n",
    "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_convnext_base_model(num_classes)\n",
    "    \n",
    "    # AdamW optimizer with cosine decay + warmup\n",
    "    total_steps = len(X_tr) // BATCH_SIZE * EPOCHS\n",
    "    warmup_steps = int(total_steps * 0.15)  # 15% warmup\n",
    "    \n",
    "    lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=5e-3,  # Lower initial LR for stability\n",
    "        decay_steps=total_steps - warmup_steps,\n",
    "        alpha=1e-6\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    loss_fn = FocalLossCB(alpha_vec=weights_cb, gamma=2.0)\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    weight_path = os.path.join(MODELS_DIR, f'fold_{fold_num}_weights.weights.h5')\n",
    "    weight_paths.append(weight_path)\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            weight_path, monitor='val_accuracy', save_best_only=True, \n",
    "            save_weights_only=True, mode='max', verbose=1\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=False, verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(f'\\nðŸš€ Training fold {fold_num}...')\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    best_acc = max(history.history['val_accuracy'])\n",
    "    print(f'âœ“ Fold {fold_num} best val accuracy: {best_acc:.4f}')\n",
    "    \n",
    "    # Cleanup\n",
    "    del X_tr, X_val, y_tr, y_val, train_ds, val_ds, model, history\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "print('\\n' + '=' * 60)\n",
    "print(f'âœ“ {KFOLDS}-fold training completed!')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726d382",
   "metadata": {},
   "source": [
    "## Test & Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions across folds\n",
    "print(\"\\nðŸ”® Inference: Ensembling fold predictions...\")\n",
    "all_preds = []\n",
    "for fold_num, weight_path in enumerate(weight_paths, 1):\n",
    "    print(f\"Loading fold {fold_num} weights from: {weight_path}\")\n",
    "    model = build_convnext_base_model(num_classes)\n",
    "    model.load_weights(weight_path)\n",
    "    preds = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "    all_preds.append(preds)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# Average predictions\n",
    "ensemble_preds = np.mean(all_preds, axis=0)\n",
    "\n",
    "# Argmax to class indices, then inverse transform to labels\n",
    "pred_indices = np.argmax(ensemble_preds, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(pred_indices)\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_files,\n",
    "    'label': pred_labels\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "out_name = f'submission_convnext_base_{KFOLDS}fold.csv'\n",
    "sub_path_drive = os.path.join(SUBMISSION_DIR, out_name)\n",
    "sub_path_local = os.path.join(BASE_PATH, out_name)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "submission.to_csv(sub_path_drive, index=False)\n",
    "submission.to_csv(sub_path_local, index=False)\n",
    "print(f\"\\nâœ“ Submission saved:\")\n",
    "print(f\"  Drive: {sub_path_drive}\")\n",
    "print(f\"  Local: {sub_path_local}\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
