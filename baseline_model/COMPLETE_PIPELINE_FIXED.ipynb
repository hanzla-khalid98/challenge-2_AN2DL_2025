{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eba85ba",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: SETUP & DATA PREPARATION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f852c",
   "metadata": {},
   "source": [
    "## 1.1) Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502782b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('âœ“ Google Drive mounted successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e59e16",
   "metadata": {},
   "source": [
    "## 1.2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ce2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import sys\n",
    "import io\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'GPU Available: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fdce3",
   "metadata": {},
   "source": [
    "## 1.3) Configure Paths\n",
    "**Update BASE_PATH to match your Google Drive structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7975a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path - UPDATE THIS to match your Google Drive structure\n",
    "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/KaggleChallenge2'\n",
    "\n",
    "# Dataset paths\n",
    "DATA_ROOT = os.path.join(BASE_PATH, 'dataset')\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_ROOT, 'train_data')\n",
    "TEST_DATA_PATH = os.path.join(DATA_ROOT, 'test_data')\n",
    "LABELS_FILE = os.path.join(DATA_ROOT, 'train_labels.csv')\n",
    "\n",
    "# Output directories\n",
    "PREPROCESSED_DIR = os.path.join(BASE_PATH, 'preprocessed_data')\n",
    "MODELS_DIR = os.path.join(BASE_PATH, 'models')\n",
    "\n",
    "# Create output directories (replace if exists)\n",
    "os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Log file (tee all stdout/stderr to file + notebook output)\n",
    "LOG_FILE = os.path.join(BASE_PATH, 'pipeline_log.txt')\n",
    "\n",
    "class Tee(io.TextIOBase):\n",
    "    def __init__(self, *streams):\n",
    "        self.streams = streams\n",
    "    def write(self, data):\n",
    "        for s in self.streams:\n",
    "            s.write(data)\n",
    "        return len(data)\n",
    "    def flush(self):\n",
    "        for s in self.streams:\n",
    "            s.flush()\n",
    "\n",
    "log_fh = open(LOG_FILE, 'w', buffering=1)\n",
    "tee_stream = Tee(sys.stdout, log_fh)\n",
    "sys.stdout = tee_stream\n",
    "sys.stderr = tee_stream\n",
    "print(f'Logging to: {LOG_FILE}')\n",
    "print(f'Run started: {datetime.now()}')\n",
    "\n",
    "print('âœ“ Paths configured:')\n",
    "print(f'  Dataset: {DATA_ROOT}')\n",
    "print(f'  Preprocessed: {PREPROCESSED_DIR}')\n",
    "print(f'  Models: {MODELS_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fc45b",
   "metadata": {},
   "source": [
    "## 1.4) Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc89460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing parameters\n",
    "TARGET_SIZE = (260, 260)  # EfficientNetB2 optimal size\n",
    "VAL_SPLIT = 0.30\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_PHASE1 = 18  # Frozen backbone\n",
    "EPOCHS_PHASE2 = 20  # Fine-tuning\n",
    "INITIAL_LR = 1e-3\n",
    "FINETUNE_LR = 1e-4\n",
    "UNFREEZE_LAYERS = 120  # Number of layers to unfreeze for fine-tuning\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "print('âœ“ Configuration:')\n",
    "print(f'  Image size: {TARGET_SIZE}')\n",
    "print(f'  Validation split: {VAL_SPLIT*100}%')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Phase 1 epochs: {EPOCHS_PHASE1} (frozen)')\n",
    "print(f'  Phase 2 epochs: {EPOCHS_PHASE2} (fine-tune)')\n",
    "print(f'  Label smoothing: {LABEL_SMOOTHING}')\n",
    "print(f'  Weight decay: {WEIGHT_DECAY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f692b9e",
   "metadata": {},
   "source": [
    "## 1.5) Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "print('='*60)\n",
    "print('TRAINING LABELS')\n",
    "print('='*60)\n",
    "print(f'\\nDataFrame shape: {labels_df.shape}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "print(labels_df.head())\n",
    "\n",
    "print(f'\\n\\nClass distribution:')\n",
    "class_counts = labels_df['label'].value_counts()\n",
    "print(class_counts)\n",
    "print(f'\\nClass balance ratio: {class_counts.max() / class_counts.min():.2f}:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0e790",
   "metadata": {},
   "source": [
    "## 1.6) Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215208bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print('='*60)\n",
    "print('LABEL ENCODING')\n",
    "print('='*60)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'\\nClass mapping:')\n",
    "for idx, name in enumerate(class_names):\n",
    "    count = (labels_df['label_encoded'] == idx).sum()\n",
    "    print(f'  {idx}: {name} ({count} samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e021cc4",
   "metadata": {},
   "source": [
    "## 1.7) Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split\n",
    "train_df, val_df = train_test_split(\n",
    "    labels_df,\n",
    "    test_size=VAL_SPLIT,\n",
    "    stratify=labels_df['label_encoded'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print('='*60)\n",
    "print('TRAIN/VALIDATION SPLIT')\n",
    "print('='*60)\n",
    "print(f'Training samples: {len(train_df)}')\n",
    "print(f'Validation samples: {len(val_df)}')\n",
    "\n",
    "print(f'\\n\\nTraining class distribution:')\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(f'\\n\\nValidation class distribution:')\n",
    "print(val_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878d7d5",
   "metadata": {},
   "source": [
    "## 1.8) Define Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, target_size=TARGET_SIZE):\n",
    "    \"\"\"\n",
    "    Load and preprocess image with EfficientNet preprocessing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Resize\n",
    "        image = image.resize(target_size, Image.BILINEAR)\n",
    "        \n",
    "        # Convert to array\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        \n",
    "            preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
    "            image = preprocess_input(image)\n",
    "        \n",
    "        return image, True\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {image_path}: {e}')\n",
    "        return None, False\n",
    "\n",
    "print('âœ“ Preprocessing function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664812d",
   "metadata": {},
   "source": [
    "## 1.9) Preprocess Training Data\n",
    "This will take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24316df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PREPROCESSING TRAINING DATA')\n",
    "print('='*60)\n",
    "\n",
    "# Initialize arrays\n",
    "X_train = np.zeros((len(train_df), *TARGET_SIZE, 3), dtype=np.float32)\n",
    "y_train = np.zeros(len(train_df), dtype=np.int32)\n",
    "\n",
    "failed_count = 0\n",
    "\n",
    "# Process each training image\n",
    "for array_idx, (_, row) in enumerate(tqdm(train_df.iterrows(), total=len(train_df), desc='Processing')):\n",
    "    image_path = os.path.join(TRAIN_DATA_PATH, row['sample_index'])\n",
    "    image, success = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    if success:\n",
    "        X_train[array_idx] = image\n",
    "        y_train[array_idx] = row['label_encoded']\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f'\\nâœ“ Training data preprocessed')\n",
    "print(f'  Successful: {len(train_df) - failed_count}')\n",
    "print(f'  Failed: {failed_count}')\n",
    "print(f'  X_train shape: {X_train.shape}')\n",
    "print(f'  y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a40aa",
   "metadata": {},
   "source": [
    "## 1.10) Preprocess Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PREPROCESSING VALIDATION DATA')\n",
    "print('='*60)\n",
    "\n",
    "# Initialize arrays\n",
    "X_val = np.zeros((len(val_df), *TARGET_SIZE, 3), dtype=np.float32)\n",
    "y_val = np.zeros(len(val_df), dtype=np.int32)\n",
    "\n",
    "failed_count = 0\n",
    "\n",
    "# Process each validation image\n",
    "for array_idx, (_, row) in enumerate(tqdm(val_df.iterrows(), total=len(val_df), desc='Processing')):\n",
    "    image_path = os.path.join(TRAIN_DATA_PATH, row['sample_index'])\n",
    "    image, success = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    if success:\n",
    "        X_val[array_idx] = image\n",
    "        y_val[array_idx] = row['label_encoded']\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f'\\nâœ“ Validation data preprocessed')\n",
    "print(f'  Successful: {len(val_df) - failed_count}')\n",
    "print(f'  Failed: {failed_count}')\n",
    "print(f'  X_val shape: {X_val.shape}')\n",
    "print(f'  y_val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c7d84",
   "metadata": {},
   "source": [
    "## 1.11) Convert Labels to One-Hot & Save Data\n",
    "**One-hot encoding is needed for label smoothing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot for label smoothing compatibility\n",
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_onehot = keras.utils.to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('SAVING PREPROCESSED DATA')\n",
    "print('='*60)\n",
    "\n",
    "# Save arrays (overwrite if exists)\n",
    "np.save(os.path.join(PREPROCESSED_DIR, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(PREPROCESSED_DIR, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(PREPROCESSED_DIR, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(PREPROCESSED_DIR, 'y_val.npy'), y_val)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'target_size': TARGET_SIZE,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names.tolist(),\n",
    "    'train_size': len(train_df),\n",
    "    'val_size': len(val_df),\n",
    "    'val_split': VAL_SPLIT,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "with open(os.path.join(PREPROCESSED_DIR, 'metadata.pkl'), 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(os.path.join(PREPROCESSED_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print('âœ“ All data saved to:', PREPROCESSED_DIR)\n",
    "print('  - X_train.npy')\n",
    "print('  - X_val.npy')\n",
    "print('  - y_train.npy')\n",
    "print('  - y_val.npy')\n",
    "print('  - metadata.pkl')\n",
    "print('  - label_encoder.pkl')\n",
    "\n",
    "print(f'\\nâœ“ Labels converted to one-hot encoding')\n",
    "print(f'  y_train_onehot shape: {y_train_onehot.shape}')\n",
    "print(f'  y_val_onehot shape: {y_val_onehot.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ad815",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: MODEL BUILDING & TRAINING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdc2ac",
   "metadata": {},
   "source": [
    "## 2.1) Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b01479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print('='*60)\n",
    "print('CLASS WEIGHTS (for imbalanced data)')\n",
    "print('='*60)\n",
    "for class_idx, weight in class_weights_dict.items():\n",
    "    class_name = class_names[class_idx]\n",
    "    count = np.sum(y_train == class_idx)\n",
    "    print(f'{class_name:20s} (class {class_idx}): weight={weight:.3f}, count={count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7723a9",
   "metadata": {},
   "source": [
    "## 2.2) Build Model with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB2 model with:\n",
    "    - Data augmentation\n",
    "    - Transfer learning\n",
    "    - Strong classification head\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Data augmentation (applied during training only)\n",
    "    x = layers.RandomFlip(\"horizontal_and_vertical\")(inputs)\n",
    "    x = layers.RandomRotation(0.12)(x)\n",
    "    x = layers.RandomZoom(0.12)(x)\n",
    "    x = layers.RandomContrast(0.15)(x)\n",
    "    x = layers.RandomTranslation(0.05, 0.05)(x)\n",
    "    x = layers.RandomBrightness(0.1)(x)\n",
    "    \n",
    "    # Load pre-trained EfficientNetB2 (frozen initially)\n",
    "    base_model = tf.keras.applications.EfficientNetB2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=x,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze initially\n",
    "    \n",
    "    # Classification head\n",
    "    x = base_model.output\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "input_shape = (*TARGET_SIZE, 3)\n",
    "model, base_model = build_model(input_shape, num_classes)\n",
    "\n",
    "print('='*60)\n",
    "print('MODEL ARCHITECTURE')\n",
    "print('='*60)\n",
    "print(f'Input shape: {input_shape}')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Total parameters: {model.count_params():,}')\n",
    "print(f'Base model (EfficientNetB2) layers: {len(base_model.layers)}')\n",
    "print(f'\\nâœ“ Model built successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa3bfc",
   "metadata": {},
   "source": [
    "## 2.3) Phase 1 - Train with Frozen Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3dcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PHASE 1: TRAINING WITH FROZEN BACKBONE')\n",
    "print('='*60)\n",
    "\n",
    "# Compile model (use CategoricalCrossentropy for label smoothing)\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=INITIAL_LR, weight_decay=WEIGHT_DECAY),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_phase1 = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(MODELS_DIR, 'best_model_phase1.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with one-hot encoded labels\n",
    "history_phase1 = model.fit(\n",
    "    X_train, y_train_onehot,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\nâœ“ Phase 1 training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72227cbe",
   "metadata": {},
   "source": [
    "## 2.4) Phase 2 - Fine-tune Top Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cedb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PHASE 2: FINE-TUNING TOP LAYERS')\n",
    "print('='*60)\n",
    "\n",
    "# Unfreeze top layers of base model\n",
    "base_model.trainable = True\n",
    "total_layers = len(base_model.layers)\n",
    "freeze_until = total_layers - UNFREEZE_LAYERS\n",
    "\n",
    "for layer in base_model.layers[:freeze_until]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f'Total base model layers: {total_layers}')\n",
    "print(f'Freezing layers: 0 to {freeze_until-1}')\n",
    "print(f'Unfreezing layers: {freeze_until} to {total_layers-1} ({UNFREEZE_LAYERS} layers)')\n",
    "\n",
    "# Recompile with lower learning rate (use CategoricalCrossentropy for label smoothing)\n",
    "model.compile(\n",
    "    optimizer=AdamW(learning_rate=FINETUNE_LR, weight_decay=WEIGHT_DECAY),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f'Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}')\n",
    "\n",
    "# Callbacks for phase 2\n",
    "callbacks_phase2 = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(MODELS_DIR, 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with one-hot encoded labels\n",
    "history_phase2 = model.fit(\n",
    "    X_train, y_train_onehot,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\nâœ“ Phase 2 training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da47a11",
   "metadata": {},
   "source": [
    "## 2.5) Save Final Model & Training Info\n",
    "**This will overwrite existing model files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55668374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('SAVING MODEL & TRAINING INFO')\n",
    "print('='*60)\n",
    "\n",
    "# Save final model (overwrite)\n",
    "model.save(os.path.join(MODELS_DIR, 'final_model.h5'))\n",
    "print('âœ“ Final model saved to: models/final_model.h5')\n",
    "print('âœ“ Best model saved to: models/best_model.h5')\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'class_names': class_names.tolist(),\n",
    "    'num_classes': num_classes,\n",
    "    'input_shape': input_shape,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_phase1': EPOCHS_PHASE1,\n",
    "    'epochs_phase2': EPOCHS_PHASE2,\n",
    "    'initial_lr': INITIAL_LR,\n",
    "    'finetune_lr': FINETUNE_LR,\n",
    "    'history_phase1': history_phase1.history,\n",
    "    'history_phase2': history_phase2.history\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, 'training_info.pkl'), 'wb') as f:\n",
    "    pickle.dump(training_info, f)\n",
    "\n",
    "print('âœ“ Training info saved to: models/training_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328288cb",
   "metadata": {},
   "source": [
    "## 2.6) Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('FINAL VALIDATION EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(os.path.join(MODELS_DIR, 'best_model.h5'))\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_accuracy = best_model.evaluate(X_val, y_val_onehot, verbose=0)\n",
    "\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)')\n",
    "\n",
    "# Per-class accuracy\n",
    "y_val_pred = best_model.predict(X_val, verbose=0)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "print(f'\\nPer-class accuracy:')\n",
    "for class_idx in range(num_classes):\n",
    "    mask = y_val == class_idx\n",
    "    if mask.sum() > 0:\n",
    "        class_acc = (y_val_pred_classes[mask] == class_idx).mean()\n",
    "        class_name = class_names[class_idx]\n",
    "        count = mask.sum()\n",
    "        print(f'  {class_name:20s}: {class_acc:.4f} ({class_acc*100:.2f}%) - {count} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0a6f7",
   "metadata": {},
   "source": [
    "## 2.7) Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88006f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "all_history = {\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss'],\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(all_history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(all_history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].axvline(x=EPOCHS_PHASE1, color='red', linestyle='--', label='Fine-tuning starts')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(all_history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(all_history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].axvline(x=EPOCHS_PHASE1, color='red', linestyle='--', label='Fine-tuning starts')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODELS_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nâœ“ Training history plot saved to: models/training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282874a8",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: GENERATE PREDICTIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6dba83",
   "metadata": {},
   "source": [
    "## 3.1) Load Test Image List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all test images\n",
    "test_files = sorted([f for f in os.listdir(TEST_DATA_PATH) if f.startswith('img_')])\n",
    "\n",
    "print('='*60)\n",
    "print('TEST DATA INFO')\n",
    "print('='*60)\n",
    "print(f'Total test images: {len(test_files)}')\n",
    "print(f'First 5: {test_files[:5]}')\n",
    "print(f'Last 5: {test_files[-5:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdfc83",
   "metadata": {},
   "source": [
    "## 3.2) Preprocess Test Images\n",
    "This may take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c393f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('PREPROCESSING TEST IMAGES')\n",
    "print('='*60)\n",
    "\n",
    "# Initialize array\n",
    "X_test = np.zeros((len(test_files), *TARGET_SIZE, 3), dtype=np.float32)\n",
    "\n",
    "failed_count = 0\n",
    "\n",
    "# Process each test image\n",
    "for idx, filename in enumerate(tqdm(test_files, desc='Processing')):\n",
    "    image_path = os.path.join(TEST_DATA_PATH, filename)\n",
    "    image, success = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    if success:\n",
    "        X_test[idx] = image\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f'\\nâœ“ Test data preprocessed')\n",
    "print(f'  Successful: {len(test_files) - failed_count}')\n",
    "print(f'  Failed: {failed_count}')\n",
    "print(f'  X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f7dfe",
   "metadata": {},
   "source": [
    "## 3.3) Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('MAKING PREDICTIONS')\n",
    "print('='*60)\n",
    "\n",
    "# Predict with best model\n",
    "predictions = best_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Decode to label names\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "print(f'\\nâœ“ Predictions complete')\n",
    "print(f'  Total predictions: {len(predicted_labels)}')\n",
    "\n",
    "# Show prediction distribution\n",
    "print(f'\\nPrediction distribution:')\n",
    "unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = (count / len(predicted_labels)) * 100\n",
    "    print(f'  {label}: {count} ({percentage:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6730ae2",
   "metadata": {},
   "source": [
    "## 3.4) Create & Save Submission File\n",
    "**This will overwrite existing submission.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3191143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_index': test_files,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# Save to CSV (overwrite if exists)\n",
    "submission_path = os.path.join(BASE_PATH, 'submission.csv')\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('SUBMISSION FILE CREATED')\n",
    "print('='*60)\n",
    "print(f'âœ“ Saved to: {submission_path}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(submission_df.head(10))\n",
    "print(f'\\nLast 10 rows:')\n",
    "print(submission_df.tail(10))\n",
    "print(f'\\nTotal rows: {len(submission_df)}')\n",
    "print(f'\\nâœ“ Ready to submit to Kaggle!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9152a27",
   "metadata": {},
   "source": [
    "---\n",
    "# PIPELINE COMPLETE! ðŸŽ‰\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download `submission.csv` from your Google Drive\n",
    "2. Submit to Kaggle\n",
    "3. Check your F1-score on the leaderboard\n",
    "\n",
    "**Expected Results:**\n",
    "- Validation Accuracy: >50%\n",
    "- Kaggle F1-Score: >0.35 (improvement from 0.2544)\n",
    "\n",
    "**Files Generated (all in your Drive):**\n",
    "- `preprocessed_data/`: X_train.npy, X_val.npy, y_train.npy, y_val.npy, metadata.pkl, label_encoder.pkl\n",
    "- `models/`: best_model.h5, final_model.h5, training_info.pkl, training_history.png\n",
    "- `submission.csv`: Ready for Kaggle submission\n",
    "- `pipeline_log.txt`: Complete execution log"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
